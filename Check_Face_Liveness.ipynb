{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Liveness - Heuristic-based solution to recognize human face by detecting eye &  lip movement and blink detection. \n",
    "Uses \"shape_predictor_68_face_landmarks.dat\" library which plotes 68 predefine points on face.Using these points tracks eye and mouth and using distance algorithm wull check if eye is blinked or not and mouth is open or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import math\n",
    "import imutils\n",
    "import face_recognition as fr\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pendulum\n",
    "from known_face_encodings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the known face names from the disk\n",
    "known_face_encodings,known_face_names = initial()\n",
    "known_face_names = [re.findall(r'.*(?=[.])',a)[0] for a in known_face_names]\n",
    "\n",
    "PATH           = \"C:/Users/sriha/Documents/My_Data/Work_Documents/Posidex/face_rec/face_rec-blink/\"\n",
    "face_detector  = cv2.CascadeClassifier(PATH + \"haarcascade_frontalface_alt.xml\")\n",
    "#open_eyes_detector   = cv2.CascadeClassifier(PATH + \"haarcascade_eye_tree_eyeglasses.xml\")\n",
    "detector       = dlib.get_frontal_face_detector()\n",
    "predictor      = dlib.shape_predictor(PATH + \"shape_predictor_68_face_landmarks.dat\")\n",
    "os.environ['OPENCV_VIDEOIO_PRIORITY_MSMF'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to determine midpoint between two coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midpoint(p1 ,p2):\n",
    "    return int((p1.x + p2.x)/2), int((p1.y + p2.y)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to calculate the distance between two coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(X,Y):\n",
    "    distance = math.sqrt((X[0] - Y[0])**2 + (X[1] - Y[1])**2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to check for eye blinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBlinking(landmarks):\n",
    "    \n",
    "    # from 68 facial landmark coordinates   \n",
    "    ##############################################\n",
    "    # Eye detection                              #\n",
    "    # Left eye points: (36, 37, 38, 39, 40, 41)  #\n",
    "    # Right eye points: (42, 43, 44, 45, 46, 47) #\n",
    "    ##############################################\n",
    "  \n",
    "        # Left Eye\n",
    "        Left_left_point    = (landmarks.part(36).x,landmarks.part(36).y)\n",
    "        Left_right_point   = (landmarks.part(39).x,landmarks.part(39).y)\n",
    "        Left_center_top    = midpoint(landmarks.part(37),landmarks.part(38))\n",
    "        Left_center_bottom = midpoint(landmarks.part(41),landmarks.part(40))\n",
    "        Left_hor_line      = cv2.line(frame, Left_left_point, Left_right_point, (0, 255, 255), 1)\n",
    "        Left_ver_line      = cv2.line(frame, Left_center_top, Left_center_bottom, (0, 255, 255), 2)\n",
    "         \n",
    "        # right Eye\n",
    "        Right_left_point    = (landmarks.part(42).x,landmarks.part(42).y)\n",
    "        Right_right_point   = (landmarks.part(45).x,landmarks.part(45).y)\n",
    "        Right_center_top    = midpoint(landmarks.part(43),landmarks.part(44))\n",
    "        Right_center_bottom = midpoint(landmarks.part(47),landmarks.part(46))\n",
    "        Right_hor_line      = cv2.line(frame, Right_left_point, Right_right_point, (0, 255, 255), 1)\n",
    "        Right_ver_line      = cv2.line(frame, Right_center_top, Right_center_bottom, (0, 255, 255), 2)\n",
    "        \n",
    "        \n",
    "        # Get the distance of horizontal and vertical lines\n",
    "        \n",
    "        Left_Y_distance = get_distance(Left_center_top,Left_center_bottom)\n",
    "        Left_X_distance = get_distance(Left_left_point,Left_right_point)\n",
    "        Left_ratio      = Left_X_distance/Left_Y_distance\n",
    "        \n",
    "        Right_Y_distance = get_distance(Right_center_top,Right_center_bottom)\n",
    "        Right_X_distance = get_distance(Right_left_point,Right_right_point)\n",
    "        Right_ratio      = Right_X_distance/Right_Y_distance\n",
    "        \n",
    "        blinking_ratio = ((Left_ratio+Right_ratio)/2)\n",
    "        \n",
    "        return(blinking_ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to check for mouth open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOpen(landmarks):\n",
    "    \n",
    "    # from 68 facial landmark coordinates \n",
    "    ##############################################\n",
    "    # Mouth detection                            #\n",
    "    # Left points: (48)                          #\n",
    "    # Right points: (54)                         #\n",
    "    # Top Lips points: (61,63)                   #\n",
    "    # Bottom Lips points: (65,67)                #\n",
    "    ##############################################\n",
    "  \n",
    "        Mouth_left_point     = (landmarks.part(48).x,landmarks.part(48).y)\n",
    "        Mouth_right_point    = (landmarks.part(54).x,landmarks.part(54).y)\n",
    "        Mouth_center_top     = midpoint(landmarks.part(61),landmarks.part(63))\n",
    "        Mouth_center_bottom  = midpoint(landmarks.part(67),landmarks.part(65))\n",
    "        hor_line             = cv2.line(frame, Mouth_left_point, Mouth_right_point, (0, 255, 255), 1)\n",
    "        ver_line             = cv2.line(frame, Mouth_center_top, Mouth_center_bottom, (0, 255, 255), 2)\n",
    "         \n",
    "        # Get the distance of horizontal and vertical lines\n",
    "        \n",
    "        Y_distance = get_distance(Mouth_center_top,Mouth_center_bottom)\n",
    "        X_distance = get_distance(Mouth_left_point,Mouth_right_point)\n",
    "        \n",
    "        try:\n",
    "            open_ratio = float(Y_distance)/float(X_distance)\n",
    "        except ZeroDivisionError:\n",
    "            open_ratio = 0\n",
    "        \n",
    "        return(open_ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to check for nose moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noseMove(landmarks):\n",
    "    \n",
    "    # from 68 facial landmark coordinates \n",
    "    ##############################################\n",
    "    # Nose  detection                            #\n",
    "    # Left points: (31)                          #\n",
    "    # Right points: (35)                         #\n",
    "    ##############################################\n",
    "  \n",
    "        Nose_left_point     = (landmarks.part(31).x,landmarks.part(31).y)\n",
    "        Nose_right_point    = (landmarks.part(35).x,landmarks.part(35).y)\n",
    "        hor_line            = cv2.line(frame, Nose_left_point, Nose_right_point, (0, 255, 255), 1)\n",
    "         \n",
    "        # Get the distance of horizontal and vertical lines\n",
    "        \n",
    "        X = Nose_right_point[0] - Nose_left_point [0] \n",
    "        Y = Nose_left_point [1] - Nose_right_point[1]\n",
    "        \n",
    "        if (Y < 0):\n",
    "            Nose_movement = 'Y'\n",
    "        else:\n",
    "            Nose_movement = 'N'\n",
    "\n",
    "        return(Nose_movement)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceMove(landmarks):\n",
    "    \n",
    "    # from 68 facial landmark coordinates \n",
    "    ##############################################\n",
    "    # Face  detection                            #\n",
    "    # Left points: (1)                          #\n",
    "    # Right points: (15)                         #\n",
    "    ##############################################\n",
    "  \n",
    "        Face_left_point     = (landmarks.part(1).x,landmarks.part(1).y)\n",
    "        Face_right_point    = (landmarks.part(15).x,landmarks.part(15).y)\n",
    "        hor_line            = cv2.line(frame, Face_left_point, Face_right_point, (0, 255, 255), 1)\n",
    "         \n",
    "        # Get the distance of horizontal and vertical lines\n",
    "        \n",
    "        X = Face_right_point[0] - Face_left_point [0] \n",
    "        Y = Face_left_point [1] - Face_right_point[1]\n",
    "        \n",
    "        if (Y < 0):\n",
    "            Face_movement = 'Y'\n",
    "        else:\n",
    "            Face_movement = 'N'\n",
    "            \n",
    "        return(Face_movement)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Live face with known face encodings and get identity having the closest distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check is unknown face is a match in the known face(s)\n",
    "# if distance bwtween unknown and known face is <= tolerance is a match\n",
    "\n",
    "def getIdentity(unknown_image,Upsample):\n",
    "    \n",
    "    global result \n",
    "    global Known_face_match\n",
    "    global min_distance\n",
    "    \n",
    "    name               = \"NoMatch\"\n",
    "    \n",
    "    # resize, convert and align\n",
    "    unknown_image   = imutils.resize(unknown_image, width=400)\n",
    "    gray            = cv2.cvtColor(unknown_image, cv2.COLOR_BGR2GRAY) \n",
    "    clahe           = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(5,8)) # Divide image into small blocks called “tiles” (Size is 8x8 by default in OpenCV)\n",
    "    gray            = clahe.apply(gray)\n",
    "    gray            = cv2.bilateralFilter(gray,5,75,75) # bilateral filter d = 15 and sigmaColor = sigmaSpace = 75. The greater its value, the more further pixels will mix together, given that their colors lie within the sigmaColor range\n",
    "    rects           = hog_face_detector(gray, Upsample)\n",
    "    \n",
    "    if rects:\n",
    "        \n",
    "        # for each detected face\n",
    "        for rect in rects:\n",
    "            unknown_image          = fa.align(unknown_image, gray, rect)\n",
    "            face_locations         = fr.face_locations(unknown_image,model=\"hog\")   \n",
    "            #face_locations         = fr.face_locations(unknown_image,model=\"cnn\")   \n",
    "            unknown_face_encodings = fr.face_encodings(unknown_image, face_locations) # Encoding one face since each image only has one face\n",
    "            \n",
    "        for (top, right, bottom, left), unknown_face_encoding in zip(face_locations, unknown_face_encodings):\n",
    "\n",
    "            matches          = fr.compare_faces(known_face_encodings, unknown_face_encoding,tolerance=0.6)\n",
    "            distance         = fr.face_distance(known_face_encodings, unknown_face_encoding)\n",
    "            distance         = list(np.around(np.array(distance),2))\n",
    "            best_match_index = np.argmin(distance)   # Get smallest distance between the two faces\n",
    "            min_distance     = distance[best_match_index]\n",
    "            \n",
    "            if matches[best_match_index]:\n",
    "                result           = 'Match '\n",
    "                Known_face_match = known_face_names[best_match_index]   \n",
    "            else:\n",
    "                result            = \"NoMatch\"\n",
    "                Known_face_match  = \"UnKnown_Face\"\n",
    "        \n",
    "    else:\n",
    "        result            = \"NoMatch\"\n",
    "        Known_face_match  = \"UnKnown_Face\"\n",
    "        min_distance      = 0 \n",
    "        \n",
    "\n",
    "    return result,Known_face_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Live Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mopen = 'N'\n",
    "blink = 'N'\n",
    "start = 'N'\n",
    "blinkRatio = 0\n",
    "mouthRatio = 0\n",
    "overlay       = cv2.imread('C:/Users/sriha/Desktop/blink.jpg')\n",
    "overlay       = cv2.resize(overlay,(800, 500))\n",
    "video         = cv2.VideoCapture(0)  # Read the image from camera\n",
    "\n",
    "while True:\n",
    "    \n",
    "    success, frame  = video.read()\n",
    "    if not success: \n",
    "        print('error')\n",
    "        break\n",
    "        \n",
    "    frame   = cv2.resize(frame, (800, 500))\n",
    "    gray    = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces   = detector(gray)\n",
    "    persons = face_detector.detectMultiScale(gray,scaleFactor=1.3,minNeighbors =5,minSize=(50, 50),flags=cv2.CASCADE_SCALE_IMAGE) \n",
    "    \n",
    "    for person in persons:\n",
    "        (x, y, w, h) = person\n",
    "            \n",
    "        for face in faces:\n",
    "            landmarks     = predictor(gray,face)\n",
    "            mouthRatio    = isOpen(landmarks)\n",
    "            blinkRatio    = isBlinking(landmarks)\n",
    "            noseMovement  = noseMove(landmarks)\n",
    "            faceMovement  = faceMove(landmarks)\n",
    "            \n",
    "\n",
    "            if (blinkRatio > 6.2):\n",
    "                cv2.putText(frame, 'Blink detected', (100,112), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 2)\n",
    "                blink = 'Y'\n",
    "\n",
    "            if (mouthRatio > 0.2):\n",
    "                cv2.putText(frame, 'Mouth open', (100,112), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 2)\n",
    "                mopen = 'Y'\n",
    "            \n",
    "            if (noseMovement == 'Y'):\n",
    "                cv2.putText(frame, 'Nose Movement', (100,112), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 2)\n",
    "            \n",
    "            if (faceMovement == 'Y'):\n",
    "                cv2.putText(frame, 'Face Movement', (100,112), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 2)\n",
    "            \n",
    "            \n",
    "            cv2.putText(frame,pendulum.now().to_day_datetime_string(),(550,20), cv2.FONT_HERSHEY_PLAIN,1,(0,0,0),2)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            #if ((blink == 'Y') or (mopen=='Y') or (noseMovement == 'Y') or (faceMovement == 'Y')):\n",
    "            if ((blink == 'Y') or (mopen=='Y') or (noseMovement == 'Y')):\n",
    "                \n",
    "                if (start=='N'):\n",
    "                    starttime        = time.time()\n",
    "                    result,knownface = getIdentity(frame,Upsample=1)\n",
    "                    start='Y'\n",
    "                \n",
    "                y = y - 15 if y - 15 > 15 else y + 15\n",
    "                #frame  = cv2.resize(frame, (800, 500))\n",
    "                output = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
    "                cv2.putText(output, knownface, (x, y), cv2.FONT_HERSHEY_SIMPLEX,1, (0, 0, 255), 3)\n",
    "                cv2.putText(output, 'Congratulations.Your new Image accepted.', (40,70), cv2.FONT_HERSHEY_SIMPLEX,1, (0, 0, 0), 3)\n",
    "                cv2.imshow('Face', output)\n",
    "                \n",
    "            else:\n",
    "                cv2.putText(frame, 'UnKnown', (x, y-5), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 3)\n",
    "                cv2.imshow('Face', frame)\n",
    "\n",
    "    if ((blink == 'Y') or (mopen=='Y') or (noseMovement == 'Y')):\n",
    "        if ((time.time()-starttime) > 10):\n",
    "            break\n",
    "    \n",
    "    if (cv2.waitKey(1)==13):   # 13 is ASCII code for Enter Key\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Donot delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mopen = 'N'\n",
    "blink = 'N'\n",
    "start = 'N'\n",
    "overlay       = cv2.imread('C:/Users/sriha/Desktop/blink.jpg')\n",
    "overlay       = cv2.resize(overlay,(800, 500))\n",
    "video         = cv2.VideoCapture(0)  # Read the image from camera\n",
    "\n",
    "while True:\n",
    "    \n",
    "    success, frame  = video.read()\n",
    "    if not success: \n",
    "        print('error')\n",
    "        break\n",
    "        \n",
    "    frame   = cv2.resize(frame, (800, 500))\n",
    "    gray    = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces   = detector(gray)\n",
    "    persons = face_detector.detectMultiScale(gray,scaleFactor=1.3,minNeighbors =5,minSize=(50, 50),flags=cv2.CASCADE_SCALE_IMAGE) \n",
    "    \n",
    "    for person in persons:\n",
    "        (x, y, w, h) = person\n",
    "            \n",
    "        for face in faces:\n",
    "            landmarks   = predictor(gray,face)\n",
    "            mouthRatio  = isOpen(landmarks)\n",
    "            blinkRatio  = isBlinking(landmarks)\n",
    "\n",
    "            if (blinkRatio > 5.7):\n",
    "                cv2.putText(frame, 'Blinking', (80,112), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 2)\n",
    "                blink = 'Y'\n",
    "\n",
    "            if (mouthRatio > 0.2):\n",
    "                cv2.putText(frame, 'OPEN', (100,112), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 2)\n",
    "                mopen = 'Y'\n",
    "             \n",
    "            cv2.putText(frame,pendulum.now().to_day_datetime_string(),(390,20), cv2.FONT_HERSHEY_PLAIN,1,(255,255,255),2)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            if ((blink == 'Y') or (mopen=='Y')):\n",
    "                \n",
    "                if (start=='N'):\n",
    "                    starttime        = time.time()\n",
    "                    result,knownface = getIdentity(frame,Upsample=1)\n",
    "                    start='Y'\n",
    "                \n",
    "                y = y - 15 if y - 15 > 15 else y + 15\n",
    "                #frame  = cv2.resize(frame, (800, 500))\n",
    "                output = cv2.addWeighted(frame, 0.7, overlay, 0.3, 0)\n",
    "                cv2.putText(output, 'Name:'+ knownface, (x, y), cv2.FONT_HERSHEY_SIMPLEX,1, (0, 255, 0), 2)\n",
    "                cv2.putText(output, 'Congratulations.Your new Image accepted.', (40,135), cv2.FONT_HERSHEY_SIMPLEX,1, (255,255,255), 3)\n",
    "                cv2.imshow('Face', output)\n",
    "                \n",
    "            else:\n",
    "                cv2.putText(frame, 'Name:UnKnown', (x, y-5), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "                cv2.imshow('Face', frame)\n",
    "\n",
    "    if ((blink == 'Y') or (mopen=='Y')):\n",
    "        if ((time.time()-starttime) > 15):\n",
    "            break\n",
    "    \n",
    "    if (cv2.waitKey(1)==13):   # 13 is ASCII code for Enter Key\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    img,blink,mopen = show(video,mopen)\n",
    "    \n",
    "    if ((time.time()-starttime) > 5):\n",
    "        cv2.putText(img, 'App will Close in 10 sec', (80,112), cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,0), 2)\n",
    "    if ((time.time()-starttime) > 35):\n",
    "        break\n",
    "    \n",
    "    cv2.putText(img,pendulum.now().to_day_datetime_string(),(390,20), cv2.FONT_HERSHEY_PLAIN,1,(255,255,255),2)\n",
    "    img  = cv2.resize(img, (800, 500))\n",
    "    \n",
    "    if (blink == 'Y') or (mopen == 'Y'):\n",
    "        \n",
    "        output  = cv2.addWeighted(img, 0.7, overlay, 0.3, 0)\n",
    "        cv2.imshow('Face', output)\n",
    "    else:\n",
    "        cv2.imshow('Face', img)\n",
    "    \n",
    "    if (cv2.waitKey(1)==13):   # 13 is ASCII code for Enter Key\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH                 = \"C:/Users/sriha/Documents/My_Data/Work_Documents/Posidex/face_rec/face_rec-blink/\"\n",
    "os.environ['OPENCV_VIDEOIO_PRIORITY_MSMF'] = '0'\n",
    "cascPath  = PATH + \"haarcascade_frontalface_default.xml\"\n",
    "eyePath   = PATH + \"haarcascade_eye.xml\"\n",
    "smilePath = PATH + \"haarcascade_smile.xml\"\n",
    "\n",
    "faceCascade  = cv2.CascadeClassifier(cascPath)\n",
    "eyeCascade   = cv2.CascadeClassifier(eyePath)\n",
    "smileCascade = cv2.CascadeClassifier(smilePath)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1, minNeighbors=5,minSize=(30, 30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        if w > 250 :\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 3)\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "            cv2.putText(frame,'Face',(x, y), font, 2,(255,0,0),5)\n",
    "            \n",
    "    smile = smileCascade.detectMultiScale(roi_gray,scaleFactor= 1.16,minNeighbors=35,minSize=(25, 25),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    for (sx, sy, sw, sh) in smile:\n",
    "        cv2.rectangle(roi_color, (sh, sy), (sx+sw, sy+sh), (255, 0, 0), 2)\n",
    "        cv2.putText(frame,'Smile',(x + sx,y + sy), 1, 1, (0, 255, 0), 1)\n",
    "        \n",
    "    eyes = eyeCascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "        cv2.putText(frame,'Eye',(x + ex,y + ey), 1, 1, (0, 255, 0), 1)\n",
    "    \n",
    "    cv2.putText(frame,'Number of Faces : ' + str(len(faces)),(40, 40), font, 1,(255,0,0),2) \n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    if (cv2.waitKey(1)==13):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
